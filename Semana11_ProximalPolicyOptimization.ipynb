{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MaxMitre/DeepLearning/blob/main/Semana11_ProximalPolicyOptimization.ipynb)"
      ],
      "metadata": {
        "id": "tEL5lxhjJzej"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Proximal Policy Optimization"
      ],
      "metadata": {
        "id": "cl_xdapBZ4lM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install xvfb\n",
        "!pip install pyvirtualdisplay > /dev/null 2>&1\n",
        "!pip install git+https://github.com/tensorflow/docs > /dev/null 2>&1"
      ],
      "metadata": {
        "id": "p3G6UvLW3tBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show tensorflow"
      ],
      "metadata": {
        "id": "BdS-4cx-ReT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Justificación\n",
        "\n",
        "Alguno de los métodos vistos anteriormente pueden tener pequeños defectos o falncias, entre los problemas que estos pueden presentar se encuentran:\n",
        "\n",
        "1. **Inestabilidad en las actualización de la política**: Dependiendo de los tamaños de lo gradientes esto puede presentar ligeros problemas.\n",
        "\n",
        "2. **Ineficiencia del uso de datos**: Actualizamos sin importar el historial de los datos, \"se usan y desechan en la misma iteración.\""
      ],
      "metadata": {
        "id": "rAZEA1a7aDxL"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udtLxTQyiNUz"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import gym\n",
        "import tensorflow_probability as tfp\n",
        "import tensorflow.keras.losses as kls\n",
        "from tensorflow.compat.v1.losses import mean_squared_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Idea\n",
        "\n",
        "Utilizar el como cambian las políticas para generar una solución a la iteración de la política.\n",
        "\n",
        "En policy gradient, utilizamos la siguiente función objetivo (la diferenciación de ésta nos da la regla de actualización de los parámetros)\n",
        "$$\n",
        "L^{PG} = \\hat{\\mathbb{E}}_t [ log \\, \\pi_{\\theta}(a_t \\vert s_t) \\hat{A}_t]\n",
        "$$\n",
        "\n",
        "Ahora usaremos el cociente $r_t$ que me dice el cambio que hay en la política al modificar el parámetro $\\theta$:\n",
        "\n",
        "$$\n",
        "\\hat{\\mathbb{E}}_t [ \\dfrac{\\pi_{\\theta}(a_t \\vert s_t)}{\\pi_{\\theta_{old}}(a_t \\vert s_t)} \\, \\hat{A}_t] = \\hat{\\mathbb{E}}_t [r_t(\\theta) \\hat{A}_t]\n",
        "$$\n",
        "\n",
        "Pero además, restringiremos este valor por si es muy grande, ya que eso podría llevar a una política que se actualice con gradientes muy grandes, lo que daría problemas. lo hacemos del siguiente modo:\n",
        "\n",
        "$$\n",
        "L^{CLIP} (\\theta) = \\hat{\\mathbb{E}}_t [ min(r_t(\\theta) \\hat{A}_t , clip(r_t(\\theta), 1 - \\epsilon, 1+ \\epsilon) \\hat{A}_t )]\n",
        "$$\n",
        "\n",
        "Este será un auxiliar, el modo final de la función objetivo tendrá 2 partes mas, una referente al cambio en la función de valor (como la usada en Actor-Critic) y una tercera parte que involucra la entropía de la política\n",
        "\n",
        "**Pregunta**: ¿Que reprensenta la entropía de una política?\n",
        "\n"
      ],
      "metadata": {
        "id": "z1MA_u_Sa-sc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "probs = [0.1,0.2,0.9,0]"
      ],
      "metadata": {
        "id": "qyxSkE0jTRpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entropia=0\n",
        "for i in probs:\n",
        "  if i!=0:\n",
        "    entropia -= i*np.log(i)\n",
        "print(entropia)"
      ],
      "metadata": {
        "id": "lMjM28R5TWYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs = [0.25,0.25,0.25,0.25]"
      ],
      "metadata": {
        "id": "w3DLpIEJTze1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entropia=0\n",
        "for i in probs:\n",
        "  if i!=0:\n",
        "    entropia -= i*np.log(i)\n",
        "print(entropia)"
      ],
      "metadata": {
        "id": "v-HtzppzT3dn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La forma final de nuestra función objetivo se muestra a continuación:\n",
        "\n",
        "$$\n",
        "L_t{}^{CLIP + VF + S} (\\theta) = \\hat{\\mathbb{E}}_t  [ L_{t}^{CLIP} (\\theta) - c_1 L_{t}^{VF} (\\theta) - c_2 S[\\pi_{\\theta}] (s_t) ]\n",
        "$$"
      ],
      "metadata": {
        "id": "2VcLiVX6VcbR"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4X3S4uD_m-e_"
      },
      "source": [
        "env= gym.make(\"CartPole-v0\")\n",
        "low = env.observation_space.low\n",
        "high = env.observation_space.high"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Redes neuronales\n",
        "\n",
        "Critic: Regresa la función de valor\n",
        "\n",
        "Actor: Regresa la acción"
      ],
      "metadata": {
        "id": "hCSBQY7Omoah"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAZ_YRduimE0"
      },
      "source": [
        "class critic(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.d1 = tf.keras.layers.Dense(128,activation='relu')\n",
        "    self.v = tf.keras.layers.Dense(1, activation = None)\n",
        "\n",
        "  def call(self, input_data):\n",
        "    x = self.d1(input_data)\n",
        "    v = self.v(x)\n",
        "    return v\n",
        "\n",
        "\n",
        "class actor(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.d1 = tf.keras.layers.Dense(128,activation='relu')\n",
        "    self.a = tf.keras.layers.Dense(2,activation='softmax')\n",
        "\n",
        "  def call(self, input_data):\n",
        "    x = self.d1(input_data)\n",
        "    a = self.a(x)\n",
        "    return a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definición del agente y sus métodos"
      ],
      "metadata": {
        "id": "dEXNyHlKm35c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Actor Loss:\n",
        "\n",
        "- Actor loss toma las probabilidades, acciones, ventajas actuales, probabilidades viejas, y pérdida de \"critic\" (la de la función de valor) como entradas.\n",
        "\n",
        "- Primero, se calculan las entropías y medias.\n",
        "\n",
        "- Luego, pasamos por las probabilidades, ventajas y probabilidades viejas para calcular los cocientes, el clip del cociente y los pegamos al final de listas.\n",
        "\n",
        "- Luego, calculamos la pérdida. Recordemos que aquí es gradiente ascendiente porque buscamos la política que nos de un valor objetivo mas grande."
      ],
      "metadata": {
        "id": "_DIA-tBOnLuB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0-5_oxXmwT9"
      },
      "source": [
        "class agent():\n",
        "    def __init__(self, gamma = 0.99):\n",
        "        self.gamma = gamma\n",
        "        # self.a_opt = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
        "        # self.c_opt = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
        "        self.a_opt = tf.keras.optimizers.Adam(learning_rate=7e-3)\n",
        "        self.c_opt = tf.keras.optimizers.Adam(learning_rate=7e-3)\n",
        "        self.actor = actor()\n",
        "        self.critic = critic()\n",
        "        self.clip_pram = 0.2\n",
        "\n",
        "\n",
        "    def act(self,state):\n",
        "        prob = self.actor(np.array([state]))\n",
        "        prob = prob.numpy()\n",
        "        dist = tfp.distributions.Categorical(probs=prob, dtype=tf.float32)\n",
        "        action = dist.sample()\n",
        "        return int(action.numpy()[0])\n",
        "\n",
        "\n",
        "\n",
        "    def actor_loss(self, probs, actions, adv, old_probs, closs):\n",
        "\n",
        "        probability = probs\n",
        "        entropy = tf.reduce_mean(tf.math.negative(tf.math.multiply(probability,tf.math.log(probability))))\n",
        "        #print(probability)\n",
        "        #print(entropy)\n",
        "        sur1 = []\n",
        "        sur2 = []\n",
        "\n",
        "        for pb, t, op,a  in zip(probability, adv, old_probs, actions):\n",
        "                        t =  tf.constant(t)\n",
        "                        #op =  tf.constant(op)\n",
        "                        #print(f\"t{t}\")\n",
        "                        #ratio = tf.math.exp(tf.math.log(pb + 1e-10) - tf.math.log(op + 1e-10))\n",
        "                        ratio = tf.math.divide(pb[a],op[a])\n",
        "                        #print(f\"ratio{ratio}\")\n",
        "                        s1 = tf.math.multiply(ratio,t)\n",
        "                        #print(f\"s1{s1}\")\n",
        "                        s2 =  tf.math.multiply(tf.clip_by_value(ratio, 1.0 - self.clip_pram, 1.0 + self.clip_pram),t)\n",
        "                        #print(f\"s2{s2}\")\n",
        "                        sur1.append(s1)\n",
        "                        sur2.append(s2)\n",
        "\n",
        "        sr1 = tf.stack(sur1)\n",
        "        sr2 = tf.stack(sur2)\n",
        "\n",
        "        #closs = tf.reduce_mean(tf.math.square(td))\n",
        "        loss = tf.math.negative(tf.reduce_mean(tf.math.minimum(sr1, sr2)) - closs + 0.001 * entropy)\n",
        "        #print(loss)\n",
        "        return loss\n",
        "\n",
        "    def learn(self, states, actions,  adv , old_probs, discnt_rewards):\n",
        "        discnt_rewards = tf.reshape(discnt_rewards, (len(discnt_rewards),))\n",
        "        adv = tf.reshape(adv, (len(adv),))\n",
        "\n",
        "        old_p = old_probs\n",
        "\n",
        "        old_p = tf.reshape(old_p, (len(old_p),2))\n",
        "        with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\n",
        "            p = self.actor(states, training=True)\n",
        "            v =  self.critic(states,training=True)\n",
        "            v = tf.reshape(v, (len(v),))\n",
        "            td = tf.math.subtract(discnt_rewards, v)\n",
        "            c_loss = 0.5 * mean_squared_error(discnt_rewards, v)\n",
        "            a_loss = self.actor_loss(p, actions, adv, old_probs, c_loss)\n",
        "\n",
        "        grads1 = tape1.gradient(a_loss, self.actor.trainable_variables)\n",
        "        grads2 = tape2.gradient(c_loss, self.critic.trainable_variables)\n",
        "        self.a_opt.apply_gradients(zip(grads1, self.actor.trainable_variables))\n",
        "        self.c_opt.apply_gradients(zip(grads2, self.critic.trainable_variables))\n",
        "        return a_loss, c_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3ariUMAZgrE"
      },
      "source": [
        "def test_reward(env):\n",
        "  total_reward = 0\n",
        "  state = env.reset()\n",
        "  done = False\n",
        "  while not done:\n",
        "    action = np.argmax(agentoo7.actor(np.array([state])).numpy())\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    state = next_state\n",
        "    total_reward += reward\n",
        "\n",
        "  return total_reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vk4Phsi1tvS-"
      },
      "source": [
        "\n",
        "def preprocess1(states, actions, rewards, done, values, gamma):\n",
        "    g = 0\n",
        "    lmbda = 0.95\n",
        "    returns = []\n",
        "    for i in reversed(range(len(rewards))):\n",
        "       delta = rewards[i] + gamma * values[i + 1] * done[i] - values[i]\n",
        "       g = delta + gamma * lmbda * dones[i] * g\n",
        "       returns.append(g + values[i])\n",
        "\n",
        "    returns.reverse()\n",
        "    adv = np.array(returns, dtype=np.float32) - values[:-1]\n",
        "    adv = (adv - np.mean(adv)) / (np.std(adv) + 1e-10)\n",
        "    states = np.array(states, dtype=np.float32)\n",
        "    actions = np.array(actions, dtype=np.int32)\n",
        "    returns = np.array(returns, dtype=np.float32)\n",
        "    return states, actions, returns, adv\n",
        "\n",
        "\n",
        "tf.random.set_seed(336699)\n",
        "agentoo7 = agent()\n",
        "steps = 5000\n",
        "ep_reward = []\n",
        "total_avgr = []\n",
        "target = False\n",
        "best_reward = 0\n",
        "avg_rewards_list = []\n",
        "\n",
        "\n",
        "for s in range(steps):\n",
        "  if target == True:\n",
        "          break\n",
        "\n",
        "  done = False\n",
        "  state = env.reset()\n",
        "  all_aloss = []\n",
        "  all_closs = []\n",
        "  rewards = []\n",
        "  states = []\n",
        "  actions = []\n",
        "  probs = []\n",
        "  dones = []\n",
        "  values = []\n",
        "  print(\"new episod\")\n",
        "\n",
        "  for e in range(128):\n",
        "\n",
        "    action = agentoo7.act(state)\n",
        "    value = agentoo7.critic(np.array([state])).numpy()\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    dones.append(1-done)\n",
        "    rewards.append(reward)\n",
        "    states.append(state)\n",
        "    #actions.append(tf.one_hot(action, 2, dtype=tf.int32).numpy().tolist())\n",
        "    actions.append(action)\n",
        "    prob = agentoo7.actor(np.array([state]))\n",
        "    probs.append(prob[0])\n",
        "    values.append(value[0][0])\n",
        "    state = next_state\n",
        "    if done:\n",
        "      env.reset()\n",
        "\n",
        "  value = agentoo7.critic(np.array([state])).numpy()\n",
        "  values.append(value[0][0])\n",
        "  np.reshape(probs, (len(probs),2))\n",
        "  probs = np.stack(probs, axis=0)\n",
        "\n",
        "  states, actions,returns, adv  = preprocess1(states, actions, rewards, dones, values, 1)\n",
        "\n",
        "  for epocs in range(10):\n",
        "      al,cl = agentoo7.learn(states, actions, adv, probs, returns)\n",
        "      # print(f\"al{al}\")\n",
        "      # print(f\"cl{cl}\")\n",
        "\n",
        "  avg_reward = np.mean([test_reward(env) for _ in range(5)])\n",
        "  print(f\"total test reward is {avg_reward}\")\n",
        "  avg_rewards_list.append(avg_reward)\n",
        "  if avg_reward > best_reward:\n",
        "        print('best reward=' + str(avg_reward))\n",
        "        agentoo7.actor.save('model_actor_{}_{}.keras'.format(s, avg_reward))\n",
        "        agentoo7.critic.save('model_critic_{}_{}.keras'.format(s, avg_reward))\n",
        "        best_reward = avg_reward\n",
        "  if best_reward == 200:\n",
        "        target = True\n",
        "  env.reset()\n",
        "\n",
        "env.close()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFe9N8TxGVkR"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pquOE2v0Snsh"
      },
      "source": [
        "ep = [i  for i in range(len(avg_rewards_list))]\n",
        "plt.plot( range(len(avg_rewards_list)),avg_rewards_list,'b')\n",
        "plt.title(\"Avg Test Reward Vs Test Episods\")\n",
        "plt.xlabel(\"Test Episods\")\n",
        "plt.ylabel(\"Average Test Reward\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualización"
      ],
      "metadata": {
        "id": "Au0eNU9z2UY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython import display as ipythondisplay\n",
        "from PIL import Image\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "max_steps_per_episode = 100\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()\n",
        "\n",
        "\n",
        "def render_episode(env: gym.Env, model: agent, max_steps: int):\n",
        "  screen = env.render(mode='rgb_array')\n",
        "  im = Image.fromarray(screen)\n",
        "\n",
        "  images = [im]\n",
        "\n",
        "  state = tf.constant(env.reset(), dtype=tf.float32)\n",
        "  for i in range(1, max_steps + 1):\n",
        "    state = tf.expand_dims(state, 0)\n",
        "    #action_probs, _ = model(state)\n",
        "    #action = np.argmax(np.squeeze(action_probs))\n",
        "    action = np.argmax(model.actor(np.array([state])).numpy())\n",
        "\n",
        "    state, _, done, _ = env.step(action)\n",
        "    state = tf.constant(state, dtype=tf.float32)\n",
        "\n",
        "    # Render screen every 10 steps\n",
        "    if i % 10 == 0:\n",
        "      screen = env.render(mode='rgb_array')\n",
        "      images.append(Image.fromarray(screen))\n",
        "\n",
        "    if done:\n",
        "      break\n",
        "\n",
        "  return images\n",
        "\n",
        "\n",
        "# Save GIF image\n",
        "images = render_episode(env, agentoo7, max_steps_per_episode)\n",
        "image_file = 'cartpole-v0.gif'\n",
        "# loop=0: loop forever, duration=1: play each frame for 1ms\n",
        "images[0].save(\n",
        "    image_file, save_all=True, append_images=images[1:], loop=0, duration=1)"
      ],
      "metadata": {
        "id": "6zEToBu0BZ02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_docs.vis.embed as embed\n",
        "embed.embed_file(image_file)"
      ],
      "metadata": {
        "id": "l4wdvvNJBqTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Referencias\n",
        "\n",
        "- [*Proximal Policy Optimization Algorithms*, OpenAI](https://arxiv.org/pdf/1707.06347.pdf)\n",
        "- https://towardsdatascience.com/proximal-policy-optimization-ppo-with-tensorflow-2-x-89c9430ecc26"
      ],
      "metadata": {
        "id": "gdwtv9hSmThB"
      }
    }
  ]
}