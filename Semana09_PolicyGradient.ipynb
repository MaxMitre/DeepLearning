{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MaxMitre/DeepLearning/blob/main/Semana09_PolicyGradient.ipynb)"
      ],
      "metadata": {
        "id": "0u_aR4NvJKoY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4cRuHV0pwCl"
      },
      "source": [
        "# Dependencias\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDv-Sspe_HFv"
      },
      "source": [
        "%%capture\n",
        "!pip install pyglet==1.3.2\n",
        "!pip install box2d-py\n",
        "!pip install gym pyvirtualdisplay\n",
        "!apt-get install -y python-opengl ffmpeg\n",
        "!pip install tensorflow==2.3.*\n",
        "!pip install --upgrade tensorflow-probability"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install xvfb\n",
        "!pip install xvfbwrapper"
      ],
      "metadata": {
        "id": "iByGtvQAkYkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym.wrappers.record_video import RecordVideo\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import numpy as np\n",
        "import random\n",
        "import glob\n",
        "import io\n",
        "import time\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900));\n",
        "display.start();"
      ],
      "metadata": {
        "id": "WCOkscpDkEV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6PzDzUmm4ip"
      },
      "source": [
        "# Auxiliar para ver el desempeño del agente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLYC5i5cm1QP"
      },
      "source": [
        "def show_video():\n",
        "  \"\"\"Permite la grabación del entorno de Gym y lo muestra.\"\"\"\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else:\n",
        "    print(\"No se encontró el video\")\n",
        "\n",
        "def wrap_env(env):\n",
        "  env = RecordVideo(env, './video')\n",
        "  return env"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_FHpZV7vMou"
      },
      "source": [
        "gpu_list = tf.config.experimental.list_physical_devices('GPU')\n",
        "print('Número de GPUs disponibles: {}'.format(len(gpu_list)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jep-1UDmp2wB"
      },
      "source": [
        "# Iniciar el ambiente y contruir el agente Policy Gradient"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YTQSNQXDvXS"
      },
      "source": [
        "env = wrap_env(gym.make('Acrobot-v1'))\n",
        "num_features = env.observation_space.shape[0]\n",
        "num_actions = env.action_space.n\n",
        "print('Número de características del estado: {}'.format(num_features))\n",
        "print('Número de acciones posibles: {}'.format(num_actions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\\pi : A \\rightarrow \\mathbb{R}$$"
      ],
      "metadata": {
        "id": "xamijBLUMgft"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por definición tenemos que:\n",
        "\n",
        "$$J(\\pi_{\\theta}) = \\underset{\\tau \\sim \\pi_{\\theta}}{E}  [{R(\\tau)}] = \\underset{\\tau \\sim \\pi_{\\theta}}{E} [\\sum_{t=0}^{T} r(s_t, a_t)] = \\int \\pi_\\theta(\\tau) R(\\tau) d\\tau $$"
      ],
      "metadata": {
        "id": "ovyTCfsEHYQg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lo que se quiere lograr es que se actualice el parámetro $\\theta$ de la función de tal modo que:\n",
        "\n",
        "$$\\theta_{k+1} = \\theta_k + \\alpha \\left. \\nabla_{\\theta} J(\\pi_{\\theta}) \\right|_{\\theta_k}$$"
      ],
      "metadata": {
        "id": "QbmhkFKtHYOZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recordemos un pequeño truco para calcular el gradiente de una función:\n",
        "\n",
        "$$\\nabla_{\\theta} \\pi_{\\theta} (\\tau) =  \\pi_{\\theta} (\\tau) \\dfrac{\\nabla_{\\theta} \\pi_{\\theta} (\\tau)}{\\pi_{\\theta} (\\tau)} =  \\pi_{\\theta} (\\tau) \\nabla_{\\theta} log\\pi_{\\theta} (\\tau)$$"
      ],
      "metadata": {
        "id": "l0q1NrMFHYMT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con todo lo anterior, podemos obtener:"
      ],
      "metadata": {
        "id": "cM8hpm0dHYJ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\begin{align*}\n",
        "\\nabla_{\\theta} J(\\pi_{\\theta}) &= \\nabla_{\\theta} \\underset{\\tau \\sim \\pi_{\\theta}}{E}     {R(\\tau)} & \\\\\n",
        "&= \\nabla_{\\theta} \\int_{\\tau} P(\\tau|\\theta) R(\\tau) & \\text{Expand expectation} \\\\\n",
        "&= \\int_{\\tau} \\nabla_{\\theta} P(\\tau|\\theta) R(\\tau) & \\text{Bring gradient under integral} \\\\\n",
        "&= \\int_{\\tau} P(\\tau|\\theta) \\nabla_{\\theta} \\log P(\\tau|\\theta) R(\\tau) & \\text{Log-derivative trick} \\\\\n",
        "&= \\underset{\\tau \\sim \\pi_{\\theta}}{E}{\\nabla_{\\theta} \\log P(\\tau|\\theta) R(\\tau)} & \\text{Return to expectation form} \\\\\n",
        "\\therefore \\nabla_{\\theta} J(\\pi_{\\theta}) &= \\underset{\\tau \\sim \\pi_{\\theta}}{E}{\\sum_{t=0}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t |s_t) R(\\tau)} & \\text{Expression for grad-log-prob}\n",
        "\\end{align*}"
      ],
      "metadata": {
        "id": "L5zzxbyeRZVJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QSN6BVFOAF7"
      },
      "source": [
        "# Create Neural Network for Policy Gradient-based Agent\n",
        "class Network(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Network, self).__init__()\n",
        "    self.dense1 = tf.keras.layers.Dense(32, activation='relu')\n",
        "    self.out = tf.keras.layers.Dense(num_actions)\n",
        "    self.dist = tfp.distributions.Categorical\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.dense1(x)\n",
        "    logits = self.out(x)\n",
        "    action = self.dist(logits=logits).sample()\n",
        "    probs = tf.nn.softmax(logits, axis=-1)\n",
        "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "    return logits, action, probs, log_probs\n",
        "\n",
        "net = Network()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfHb7hRMqX3w"
      },
      "source": [
        "# Función para hacer un paso de entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nKGzJCmuW7S"
      },
      "source": [
        "@tf.function\n",
        "def train_step(batch_states, batch_actions, batch_returns):\n",
        "  with tf.GradientTape() as tape:\n",
        "    logits, actions, probs, log_probs = net(batch_states)\n",
        "    action_masks = tf.one_hot(batch_actions, num_actions)\n",
        "    masked_log_probs = tf.reduce_sum(action_masks * log_probs, axis=-1)\n",
        "    loss = -tf.reduce_mean(batch_returns * masked_log_probs)\n",
        "  net_gradients = tape.gradient(loss, net.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(net_gradients, net.trainable_variables))\n",
        "  return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr05OsjgqhF3"
      },
      "source": [
        "# Ver el algoritmo y ver su aprendizaje"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3BqYiRmJfl5"
      },
      "source": [
        "num_episodes = 100 # @param {type:\"integer\"}\n",
        "viz_update_freq = 50 # @param {type: \"integer\"}\n",
        "steps_per_train_step = 5000 # @param {type: \"integer\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGChb068nVK6"
      },
      "source": [
        "%%time\n",
        "\n",
        "last_100_ep_ret, text = [], ''\n",
        "batch_states, batch_actions, batch_returns = [], [], []\n",
        "for episode in range(num_episodes):\n",
        "  if episode % viz_update_freq == 0: # Needed for updating the visualization.\n",
        "    env.close()\n",
        "    env = wrap_env(gym.make('Acrobot-v1'))\n",
        "\n",
        "  # Start a new episode and reset the environment.\n",
        "  state = env.reset()\n",
        "  done, ep_rew = False, []\n",
        "  while not done:\n",
        "    state_in = np.expand_dims(state, 0)\n",
        "    # Sample action from policy and take that action in the env.\n",
        "    logits, action, probs, log_probs = net(state_in)\n",
        "    next_state, reward, done, info = env.step(action[0].numpy())\n",
        "    batch_states.append(state)\n",
        "    batch_actions.append(action[0])\n",
        "    ep_rew.append(reward)\n",
        "    state = next_state\n",
        "\n",
        "  # Create episode returns for policy gradient step.\n",
        "  episode_ret = sum(ep_rew)\n",
        "  episode_len = len(ep_rew)\n",
        "  batch_returns += [episode_ret] * episode_len\n",
        "\n",
        "  # Keep collecting experience with the current policy.\n",
        "  if len(batch_states) >= steps_per_train_step:\n",
        "    # Now that we have enough experience for this policy, train it on-policy.\n",
        "    loss = train_step(np.array(batch_states), np.array(batch_actions),\n",
        "                      np.array(batch_returns, dtype=np.float32))\n",
        "    # Print the performance of the policy.\n",
        "    ipythondisplay.clear_output()\n",
        "    text += f\"Episode: {episode}, Loss: {loss:.2f}, \"\\\n",
        "            f\"Return: {np.mean(batch_returns):.2f}\\n\"\n",
        "    print(text)\n",
        "    print('Current agent performance:')\n",
        "    show_video()\n",
        "    batch_states, batch_actions, batch_returns = [], [], []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFAC-2JRzCL3"
      },
      "source": [
        "# Ver desempeño despues de entrenarlo\n",
        "\n",
        "### Correr multiples veces para jugar el juego una y otra vez."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9r1as3xlAcd"
      },
      "source": [
        "env = wrap_env(gym.make('Acrobot-v1'))\n",
        "state = env.reset()\n",
        "ret = 0\n",
        "while True:\n",
        "  env.render()\n",
        "  state = tf.expand_dims(state, axis=0)\n",
        "  logits, action, probs, log_probs = net(state)\n",
        "  state, reward, done, info = env.step(action[0].numpy())\n",
        "  ret += reward\n",
        "  if done:\n",
        "    break\n",
        "env.close()\n",
        "print('Return on this episode: {}'.format(ret))\n",
        "show_video()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Referencias\n",
        "\n",
        "- [Medium:Introducción al aprendizaje por refuerzo](https://markelsanz14.medium.com/introducci%C3%B3n-al-aprendizaje-por-refuerzo-parte-5-pol%C3%ADticas-de-gradiente-8e92725e9c8f)\n",
        "\n",
        "- [Intro a policy Optimization](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html)"
      ],
      "metadata": {
        "id": "zifXDZH8Sz3f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convergencia en iterar políticas vs cambiar con función de valor: Teoremas"
      ],
      "metadata": {
        "id": "9-6jSSNE9ldz"
      }
    }
  ]
}