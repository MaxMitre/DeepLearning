{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MaxMitre/DeepLearning/blob/main/Semana05_2_DQN_Video.ipynb)"
      ],
      "metadata": {
        "id": "jX7pnLAN-Mjo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GeneraciÃ³n de video para ejemplo del Taxi"
      ],
      "metadata": {
        "id": "GRGeBYLznO84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyVirtualDisplay"
      ],
      "metadata": {
        "id": "FQn8-nsovgiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "il-IP1dBFS29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "from collections import namedtuple, deque\n",
        "\n",
        "import tensorflow as tf\n",
        "from pyvirtualdisplay import Display\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.losses import MSE\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import random"
      ],
      "metadata": {
        "id": "PtYJXVG4vbY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y xvfb"
      ],
      "metadata": {
        "id": "QzBrYsEDwTK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xvfbwrapper"
      ],
      "metadata": {
        "id": "rmtn_pznvzzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SR-nY7QHvQZ6"
      },
      "outputs": [],
      "source": [
        "Display(visible=False, size=(840, 480)).start()\n",
        "tf.random.set_seed(0)\n",
        "\n",
        "env = gym.make(\"Taxi-v3\")\n",
        "env.reset()\n",
        "\n",
        "MEMORY_SIZE = 100_000\n",
        "GAMMA = 0.95\n",
        "ALPHA = 0.001\n",
        "NUM_STEPS_FOR_UPDATE = 4\n",
        "\n",
        "experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "\n",
        "num_states = env.observation_space.n\n",
        "num_actions = env.action_space.n\n",
        "\n",
        "q_network = Sequential([\n",
        "    Input(shape=(num_states,)),\n",
        "    Dense(64, activation=\"relu\"),\n",
        "    Dense(64, activation=\"relu\"),\n",
        "    Dense(num_actions, activation=\"linear\")\n",
        "])\n",
        "\n",
        "target_q_network = Sequential([\n",
        "    Input(shape=(num_states,)),\n",
        "    Dense(64, activation=\"relu\"),\n",
        "    Dense(64, activation=\"relu\"),\n",
        "    Dense(num_actions, activation=\"linear\")\n",
        "])\n",
        "\n",
        "optimizer = Adam(learning_rate=ALPHA)\n",
        "\n",
        "def compute_loss(experiences, gamma, q_network, target_q_network):\n",
        "  states, actions, rewards, next_states, done_vals = experiences\n",
        "  max_qsa = tf.reduce_max(target_q_network(next_states), axis=-1)\n",
        "  y_targets = rewards + (gamma * max_qsa * (1-done_vals))\n",
        "  q_values = q_network(states)\n",
        "  q_values = tf.gather_nd(q_values, tf.stack([tf.range(q_values.shape[0]),\n",
        "                                                tf.cast(actions, tf.int32)], axis=1))\n",
        "  loss = MSE(y_targets, q_values)\n",
        "\n",
        "  return loss\n",
        "\n",
        "def update_target_network(q_network, target_q_network):\n",
        "  TAU=1e-3\n",
        "  for target_weights, q_network_weights in zip(target_q_network.weights, q_network.weights):\n",
        "    target_weights.assign(TAU * q_network_weights + (1.0-TAU) * target_weights)\n",
        "\n",
        "@tf.function\n",
        "def agent_learn(experiences, gamma, q_network, target_q_network, optimizer):\n",
        "  with tf.GradientTape() as tape:\n",
        "    loss = compute_loss(experiences, gamma, q_network, target_q_network)\n",
        "  gradients = tape.gradient(loss, q_network.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, q_network.trainable_variables))\n",
        "  update_target_network(q_network, target_q_network)\n",
        "\n",
        "def get_action(q_values, epsilon=0):\n",
        "  if random.random() > epsilon:\n",
        "    return np.argmax(q_values.numpy()[0])\n",
        "  else:\n",
        "    return random.choice(np.arange(6))\n",
        "\n",
        "def check_update_conditions(j, NUM_STEPS_FOR_UPDATE, memory_buffer):\n",
        "  if(j+1) % NUM_STEPS_FOR_UPDATE == 0 and len(memory_buffer) > 64:\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "def get_experiences(memory_buffer):\n",
        "    experiences = random.sample(memory_buffer, k=64)\n",
        "    states = tf.convert_to_tensor(np.array([e.state for e in experiences if e is not None]),dtype=tf.float32)\n",
        "    actions = tf.convert_to_tensor(np.array([e.action for e in experiences if e is not None]), dtype=tf.float32)\n",
        "    rewards = tf.convert_to_tensor(np.array([e.reward for e in experiences if e is not None]), dtype=tf.float32)\n",
        "    next_states = tf.convert_to_tensor(np.array([e.next_state for e in experiences if e is not None]),dtype=tf.float32)\n",
        "    done_vals = tf.convert_to_tensor(np.array([e.done for e in experiences if e is not None]).astype(np.uint8),\n",
        "                                     dtype=tf.float32)\n",
        "    return (states, actions, rewards, next_states, done_vals)\n",
        "\n",
        "def get_new_epsilon(epsilon):\n",
        "  E_MIN = 0.01\n",
        "  E_DECAY = 0.05\n",
        "  return max(E_MIN, E_DECAY * epsilon)\n",
        "\n",
        "def get_one_hot_encoding(state, next_state):\n",
        "\n",
        "  state_arr = np.zeros(500)\n",
        "  next_state_arr = np.zeros(500)\n",
        "\n",
        "  state_arr[state] = 1\n",
        "  next_state_arr[next_state] = 1\n",
        "\n",
        "  return state_arr, next_state_arr\n",
        "\n",
        "\n",
        "from gym.envs.toy_text.frozen_lake import generate_random_map\n",
        "\n",
        "def train():\n",
        "\n",
        "  NUM_EPISODES = 50000\n",
        "  MAX_TIMESTEPS = 1000\n",
        "\n",
        "  memory_buffer = deque(maxlen=MEMORY_SIZE)\n",
        "  target_q_network.set_weights(q_network.get_weights())\n",
        "\n",
        "  epsilon = 1.0\n",
        "\n",
        "  points_history = []\n",
        "\n",
        "  for i in range(NUM_EPISODES):\n",
        "\n",
        "    state = env.reset()\n",
        "    state, _ = get_one_hot_encoding(state, 0)\n",
        "    total_points = 0\n",
        "\n",
        "    for j in range(MAX_TIMESTEPS):\n",
        "\n",
        "      state_qn = np.expand_dims(state, axis=0)\n",
        "      q_values = q_network(state_qn)\n",
        "      action = get_action(q_values, epsilon)\n",
        "      next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "      _, next_state = get_one_hot_encoding(0, next_state)\n",
        "\n",
        "      memory_buffer.append(experience(state, action, reward, next_state, done))\n",
        "\n",
        "      update = check_update_conditions(j, NUM_STEPS_FOR_UPDATE, memory_buffer)\n",
        "\n",
        "      if update:\n",
        "        experiences = get_experiences(memory_buffer)\n",
        "        agent_learn(experiences, GAMMA, q_network, target_q_network, optimizer)\n",
        "\n",
        "      state = next_state.copy()\n",
        "      total_points += reward\n",
        "\n",
        "      if done:\n",
        "        break\n",
        "\n",
        "    points_history.append(total_points)\n",
        "    avg_points = np.mean(points_history[-100:])\n",
        "\n",
        "    epsilon = get_new_epsilon(epsilon)\n",
        "\n",
        "    print(f\"\\rEpisode {i+1} | Total point average of the last {100} episodes: {avg_points:.2f}\", end=\"\")\n",
        "\n",
        "    if (i+1) % 100 == 0:\n",
        "        print(f\"\\rEpisode {i+1} | Total point average of the last {100} episodes: {avg_points:.2f}\")\n",
        "\n",
        "    if(avg_points >= 8):\n",
        "      print(f\"Environment solved in {i+1} episodes!\")\n",
        "      #q_network.save('/content/drive/MyDrive/Curso-DeepLearning/Semana6/taxi_mp4/taxi_model.h5')\n",
        "      break\n",
        "\n",
        "\n",
        "#train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q_network"
      ],
      "metadata": {
        "id": "kYzLAmRtFk5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import logging\n",
        "import imageio"
      ],
      "metadata": {
        "id": "ugsn2wQryOgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#q_network.save('/content/drive/MyDrive/Curso-DeepLearning/taxi_model.h5')"
      ],
      "metadata": {
        "id": "_C3svDy8BVu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging.getLogger().setLevel(logging.ERROR)\n",
        "\n",
        "env = gym.make(\"Taxi-v3\")\n",
        "\n",
        "q_network = tf.keras.models.load_model('/content/drive/MyDrive/Curso-DeepLearning/taxi_model.h5')\n",
        "\n",
        "def get_one_hot_encoding(state, next_state):\n",
        "\n",
        "  state_arr = np.zeros(500)\n",
        "  next_state_arr = np.zeros(500)\n",
        "\n",
        "  state_arr[state] = 1\n",
        "  next_state_arr[next_state] = 1\n",
        "\n",
        "  return state_arr, next_state_arr\n",
        "\n",
        "def create_video(filename, env, q_network, fps=30):\n",
        "  video = imageio.get_writer(filename, fps=fps)\n",
        "  done = False\n",
        "  state = env.reset()\n",
        "  frame = env.render(mode=\"rgb_array\")\n",
        "  video.append_data(frame)\n",
        "  while not done:\n",
        "    state, _ = get_one_hot_encoding(state, 0)\n",
        "    state = np.expand_dims(state, axis=0)\n",
        "    q_values = q_network(state)\n",
        "    action = np.argmax(q_values.numpy()[0])\n",
        "    state, _, done, _ = env.step(action)\n",
        "    frame = env.render(mode=\"rgb_array\")\n",
        "    video.append_data(frame)\n",
        "    for k in range(20):\n",
        "      video.append_data(frame)\n",
        "\n",
        "filename = \"./taxi.mp4\"\n",
        "\n",
        "create_video(filename, env, q_network)"
      ],
      "metadata": {
        "id": "1qH7wQ5VyND5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Referencias:\n",
        "\n",
        "https://colab.research.google.com/drive/1lNKG5F1kSR-Uf2jA2yjm08qVewful5t8#scrollTo=1qH7wQ5VyND5"
      ],
      "metadata": {
        "id": "dz4rwspBmORM"
      }
    }
  ]
}