{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MaxMitre/DeepLearning/blob/main/Semana10_Actor_Critic.ipynb)"
      ],
      "metadata": {
        "id": "fp7OGKu9JnLC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Método Actor-Critic\n",
        "\n",
        "En el método Actor-Crítico, la política se conoce como el actor que propone un conjunto de posibles acciones dado un estado, y la función de valor estimado se denomina crítica , que evalúa las acciones tomadas por el actor en función de la política dada."
      ],
      "metadata": {
        "id": "suaJoAXvZA4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym\n",
        "!pip install pyglet"
      ],
      "metadata": {
        "id": "0soq8tbG_QCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install additional packages for visualization\n",
        "!sudo apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "!pip install pyvirtualdisplay > /dev/null 2>&1\n",
        "!pip install git+https://github.com/tensorflow/docs > /dev/null 2>&1"
      ],
      "metadata": {
        "id": "UaG1USEU_P_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show tensorflow"
      ],
      "metadata": {
        "id": "DluKl0G4QjEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CartPole-v0\n",
        "\n",
        "En el entorno , se une un poste a un carro que se mueve a lo largo de una pista sin fricción. El poste comienza en posición vertical y el objetivo del agente es evitar que se caiga aplicando una fuerza de -1 o +1 al carro. Se otorga una recompensa de +1 por cada paso de tiempo que el poste permanece en posición vertical. Un episodio termina cuando (1) el poste está a más de 15 grados de la vertical o (2) el carro se mueve a más de 2,4 unidades del centro."
      ],
      "metadata": {
        "id": "Exi7SfKBXgSU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El problema se considera \"resuelto\" cuando la recompensa total promedio del episodio llega a 195 en 100 intentos consecutivos."
      ],
      "metadata": {
        "id": "M9_QuIyhZJVF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65LziG8BkMWz"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import gym\n",
        "import numpy as np\n",
        "import statistics\n",
        "import tensorflow as tf\n",
        "import tqdm\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow.keras import layers\n",
        "from typing import Any, List, Sequence, Tuple\n",
        "\n",
        "\n",
        "# Create the environment\n",
        "env = gym.make(\"CartPole-v0\")\n",
        "\n",
        "# Set seed for experiment reproducibility\n",
        "seed = 42\n",
        "env.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Small epsilon value for stabilizing division operations\n",
        "eps = np.finfo(np.float32).eps.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo\n",
        "\n",
        "El actor y el crítico se modelarán utilizando una red neuronal que genera las probabilidades de acción y el valor crítico, respectivamente. Este tutorial utiliza subclases de modelos para definir el modelo.\n",
        "\n",
        "Durante el paso hacia adelante, el modelo tomará el estado como entrada y generará tanto las probabilidades de acción como el valor crítico $V$, que modela la función de valor dependiente del estado. El objetivo es entrenar un modelo que elija acciones basadas en una política $\\pi$ que maximice el rendimiento esperado.\n",
        "\n",
        "Para Cartpole-v0, hay cuatro valores que representan el estado: posición del carro, velocidad del carro, ángulo del poste y velocidad del poste, respectivamente. El agente puede realizar dos acciones para empujar el carro hacia la izquierda (0) y hacia la derecha (1) respectivamente."
      ],
      "metadata": {
        "id": "-irXFhMxZNKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ActorCritic(tf.keras.Model):\n",
        "  \"\"\"Combined actor-critic network.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      num_actions: int,\n",
        "      num_hidden_units: int):\n",
        "    \"\"\"Initialize.\"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.common = layers.Dense(num_hidden_units, activation=\"relu\")\n",
        "    self.actor = layers.Dense(num_actions)\n",
        "    self.critic = layers.Dense(1)\n",
        "\n",
        "  def call(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "    x = self.common(inputs)\n",
        "    return self.actor(x), self.critic(x)"
      ],
      "metadata": {
        "id": "n2fq76bEA5fG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_actions = env.action_space.n  # 2\n",
        "num_hidden_units = 128\n",
        "\n",
        "model = ActorCritic(num_actions, num_hidden_units)"
      ],
      "metadata": {
        "id": "sCTaJ6yRA5c5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrap OpenAI Gym's `env.step` call as an operation in a TensorFlow function.\n",
        "# This would allow it to be included in a callable TensorFlow graph.\n",
        "\n",
        "def env_step(action: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "  \"\"\"Returns state, reward and done flag given an action.\"\"\"\n",
        "\n",
        "  state, reward, done, _ = env.step(action)\n",
        "  return (state.astype(np.float32),\n",
        "          np.array(reward, np.int32),\n",
        "          np.array(done, np.int32))\n",
        "\n",
        "\n",
        "def tf_env_step(action: tf.Tensor) -> List[tf.Tensor]:\n",
        "  return tf.numpy_function(env_step, [action],\n",
        "                           [tf.float32, tf.int32, tf.int32])"
      ],
      "metadata": {
        "id": "-mEehrKQA5YT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_episode(\n",
        "    initial_state: tf.Tensor,\n",
        "    model: tf.keras.Model,\n",
        "    max_steps: int) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n",
        "  \"\"\"Runs a single episode to collect training data.\"\"\"\n",
        "\n",
        "  action_probs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
        "  values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
        "  rewards = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n",
        "\n",
        "  initial_state_shape = initial_state.shape\n",
        "  state = initial_state\n",
        "\n",
        "  for t in tf.range(max_steps):\n",
        "    # Convert state into a batched tensor (batch size = 1)\n",
        "    state = tf.expand_dims(state, 0)\n",
        "\n",
        "    # Run the model and to get action probabilities and critic value\n",
        "    action_logits_t, value = model(state)\n",
        "\n",
        "    # Sample next action from the action probability distribution\n",
        "    action = tf.random.categorical(action_logits_t, 1)[0, 0]\n",
        "    action_probs_t = tf.nn.softmax(action_logits_t)\n",
        "\n",
        "    # Store critic values\n",
        "    values = values.write(t, tf.squeeze(value))\n",
        "\n",
        "    # Store log probability of the action chosen\n",
        "    action_probs = action_probs.write(t, action_probs_t[0, action])\n",
        "\n",
        "    # Apply action to the environment to get next state and reward\n",
        "    state, reward, done = tf_env_step(action)\n",
        "    state.set_shape(initial_state_shape)\n",
        "\n",
        "    # Store reward\n",
        "    rewards = rewards.write(t, reward)\n",
        "\n",
        "    if tf.cast(done, tf.bool):\n",
        "      break\n",
        "\n",
        "  action_probs = action_probs.stack()\n",
        "  values = values.stack()\n",
        "  rewards = rewards.stack()\n",
        "\n",
        "  return action_probs, values, rewards"
      ],
      "metadata": {
        "id": "rloiho17A5V_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rendimientos esperados\n",
        "\n",
        "La secuencia de recompensas para cada paso de tiempo $t$, $\\{r_{t}\\}^{T}_{t=1}$ recopilada durante un episodio se convierte en una secuencia de rendimientos esperados $\\{G_{t}\\}^{T}_{t=1}$ en la que la suma de recompensas se toma del paso de tiempo actual $t$ a $T$ y tiempo cada la recompensa se multiplica con un factor de descuento exponencialmente decreciente $\\gamma$:\n",
        "\n",
        "$$ G_{t} = \\sum^{T}_{t'=t} \\gamma^{t'-t}r_{t'}$$\n",
        "Desde $\\gamma \\in(0,1)$, las recompensas más alejadas del paso de tiempo actual tienen menos peso.\n",
        "\n",
        "Intuitivamente, el rendimiento esperado simplemente implica que las recompensas ahora son mejores que las recompensas posteriores. En un sentido matemático, es para asegurar que la suma de las recompensas converja.\n",
        "\n",
        "Para estabilizar el entrenamiento, la secuencia resultante de retornos también está estandarizada (es decir, para tener media cero y desviación estándar unitaria)."
      ],
      "metadata": {
        "id": "zWrQAnuVbVAV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_expected_return(\n",
        "    rewards: tf.Tensor,\n",
        "    gamma: float,\n",
        "    standardize: bool = True) -> tf.Tensor:\n",
        "  \"\"\"Compute expected returns per timestep.\"\"\"\n",
        "\n",
        "  n = tf.shape(rewards)[0]\n",
        "  returns = tf.TensorArray(dtype=tf.float32, size=n)\n",
        "\n",
        "  # Start from the end of `rewards` and accumulate reward sums\n",
        "  # into the `returns` array\n",
        "  rewards = tf.cast(rewards[::-1], dtype=tf.float32)\n",
        "  discounted_sum = tf.constant(0.0)\n",
        "  discounted_sum_shape = discounted_sum.shape\n",
        "  for i in tf.range(n):\n",
        "    reward = rewards[i]\n",
        "    discounted_sum = reward + gamma * discounted_sum\n",
        "    discounted_sum.set_shape(discounted_sum_shape)\n",
        "    returns = returns.write(i, discounted_sum)\n",
        "  returns = returns.stack()[::-1]\n",
        "\n",
        "  if standardize:\n",
        "    returns = ((returns - tf.math.reduce_mean(returns)) /\n",
        "               (tf.math.reduce_std(returns) + eps))\n",
        "\n",
        "  return returns"
      ],
      "metadata": {
        "id": "MJw-6QlNA5Rh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Función de pérdida\n",
        "\n",
        "Dado que se utiliza un modelo híbrido actor-crítico, la función de pérdida elegida es una combinación de pérdidas de actor y crítico para el entrenamiento, como se muestra a continuación:\n",
        "$$ L = L_{actor} + L_{critic}$$\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "R5cIEtyEcGDS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pérdida de actores\n",
        "\n",
        "La pérdida del actor se basa en gradientes de política con el crítico como una línea de base dependiente del estado y se calcula con estimaciones de muestra única (por episodio).\n",
        "\n",
        "$$ L_{actor} = -\\sum^{T}_{t=1} \\log\\pi_{\\theta}(a_{t} | s_{t})[G(s_{t}, a_{t})  - V^{\\pi}_{\\theta}(s_{t})]$$\n",
        "\n",
        "donde:\n",
        "\n",
        "$T$ : el número de pasos de tiempo por episodio, que puede variar por episodio\n",
        "\n",
        "$s_t$ : el estado en el paso de tiempo $t$\n",
        "\n",
        "$a_t$ : acción elegida en el paso de tiempo $t$ dado estado $s$\n",
        "\n",
        "$\\pi_\\theta$ : es la política (actor) parametrizada por $\\theta$\n",
        "\n",
        "$V_{\\theta}^{\\pi}$ : es la función de valor (crítica) también parametrizada por $\\theta$\n",
        "\n",
        "$G = G_t$ : el rendimiento esperado para un estado dado, par de acciones en el paso de tiempo $t$\n",
        "\n",
        "Se agrega un término negativo a la suma ya que la idea es maximizar las probabilidades de que las acciones produzcan mayores recompensas al minimizar la pérdida combinada."
      ],
      "metadata": {
        "id": "VDXnMlBtcQTM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pérdida crítica\n",
        "\n",
        "El entrenamiento $V$ para que esté lo más cerca posible de $G$ se puede configurar como un problema de regresión con la siguiente función de pérdida:\n",
        "\n",
        "$$L_{critic} = L_{\\delta}(G, V^{\\pi}_{\\theta})$$\n",
        "\n",
        "donde $L_{\\delta}$ es la pérdida de [Huber](https://en.wikipedia.org/wiki/Huber_loss), que es menos sensible a los valores atípicos en los datos que la pérdida por error cuadrático."
      ],
      "metadata": {
        "id": "-Gyq3aSPc7se"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
        "\n",
        "def compute_loss(\n",
        "    action_probs: tf.Tensor,\n",
        "    values: tf.Tensor,\n",
        "    returns: tf.Tensor) -> tf.Tensor:\n",
        "  \"\"\"Computes the combined actor-critic loss.\"\"\"\n",
        "\n",
        "  advantage = returns - values\n",
        "\n",
        "  action_log_probs = tf.math.log(action_probs)\n",
        "  actor_loss = -tf.math.reduce_sum(action_log_probs * advantage)\n",
        "\n",
        "  critic_loss = huber_loss(values, returns)\n",
        "\n",
        "  return actor_loss + critic_loss"
      ],
      "metadata": {
        "id": "NJF5ZCL2BaCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def train_step(\n",
        "    initial_state: tf.Tensor,\n",
        "    model: tf.keras.Model,\n",
        "    optimizer: tf.keras.optimizers.Optimizer,\n",
        "    gamma: float,\n",
        "    max_steps_per_episode: int) -> tf.Tensor:\n",
        "  \"\"\"Runs a model training step.\"\"\"\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "\n",
        "    # Run the model for one episode to collect training data\n",
        "    action_probs, values, rewards = run_episode(\n",
        "        initial_state, model, max_steps_per_episode)\n",
        "\n",
        "    # Calculate expected returns\n",
        "    returns = get_expected_return(rewards, gamma)\n",
        "\n",
        "    # Convert training data to appropriate TF tensor shapes\n",
        "    action_probs, values, returns = [\n",
        "        tf.expand_dims(x, 1) for x in [action_probs, values, returns]]\n",
        "\n",
        "    # Calculating loss values to update our network\n",
        "    loss = compute_loss(action_probs, values, returns)\n",
        "\n",
        "  # Compute the gradients from the loss\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "  # Apply the gradients to the model's parameters\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "  episode_reward = tf.math.reduce_sum(rewards)\n",
        "\n",
        "  return episode_reward"
      ],
      "metadata": {
        "id": "JZxnml21BZ9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrenamiento"
      ],
      "metadata": {
        "id": "od_Z2SO8dZrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "min_episodes_criterion = 100\n",
        "max_episodes = 10000\n",
        "max_steps_per_episode = 1000\n",
        "\n",
        "# Cartpole-v0 is considered solved if average reward is >= 195 over 100\n",
        "# consecutive trials\n",
        "reward_threshold = 195\n",
        "running_reward = 0\n",
        "\n",
        "# Discount factor for future rewards\n",
        "gamma = 0.99\n",
        "\n",
        "# Keep last episodes reward\n",
        "episodes_reward: collections.deque = collections.deque(maxlen=min_episodes_criterion)\n",
        "\n",
        "with tqdm.trange(max_episodes) as t:\n",
        "  for i in t:\n",
        "    initial_state = tf.constant(env.reset(), dtype=tf.float32)\n",
        "    episode_reward = int(train_step(\n",
        "        initial_state, model, optimizer, gamma, max_steps_per_episode))\n",
        "\n",
        "    episodes_reward.append(episode_reward)\n",
        "    running_reward = statistics.mean(episodes_reward)\n",
        "\n",
        "    t.set_description(f'Episode {i}')\n",
        "    t.set_postfix(\n",
        "        episode_reward=episode_reward, running_reward=running_reward)\n",
        "\n",
        "    # Show average episode reward every 10 episodes\n",
        "    if i % 10 == 0:\n",
        "      pass # print(f'Episode {i}: average reward: {avg_reward}')\n",
        "\n",
        "    if running_reward > reward_threshold and i >= min_episodes_criterion:\n",
        "        break\n",
        "\n",
        "print(f'\\nSolved at episode {i}: average reward: {running_reward:.2f}!')"
      ],
      "metadata": {
        "id": "DMi3TTmPBZ5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install xvfb\n"
      ],
      "metadata": {
        "id": "0PutNd1vBZ3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualización"
      ],
      "metadata": {
        "id": "Da7-EOmEdb-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython import display as ipythondisplay\n",
        "from PIL import Image\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()\n",
        "\n",
        "\n",
        "def render_episode(env: gym.Env, model: tf.keras.Model, max_steps: int):\n",
        "  screen = env.render(mode='rgb_array')\n",
        "  im = Image.fromarray(screen)\n",
        "\n",
        "  images = [im]\n",
        "\n",
        "  state = tf.constant(env.reset(), dtype=tf.float32)\n",
        "  for i in range(1, max_steps + 1):\n",
        "    state = tf.expand_dims(state, 0)\n",
        "    action_probs, _ = model(state)\n",
        "    action = np.argmax(np.squeeze(action_probs))\n",
        "\n",
        "    state, _, done, _ = env.step(action)\n",
        "    state = tf.constant(state, dtype=tf.float32)\n",
        "\n",
        "    # Render screen every 10 steps\n",
        "    if i % 10 == 0:\n",
        "      screen = env.render(mode='rgb_array')\n",
        "      images.append(Image.fromarray(screen))\n",
        "\n",
        "    if done:\n",
        "      break\n",
        "\n",
        "  return images\n",
        "\n",
        "\n",
        "# Save GIF image\n",
        "images = render_episode(env, model, max_steps_per_episode)\n",
        "image_file = 'cartpole-v0.gif'\n",
        "# loop=0: loop forever, duration=1: play each frame for 1ms\n",
        "images[0].save(\n",
        "    image_file, save_all=True, append_images=images[1:], loop=0, duration=1)"
      ],
      "metadata": {
        "id": "6zEToBu0BZ02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_docs.vis.embed as embed\n",
        "embed.embed_file(image_file)"
      ],
      "metadata": {
        "id": "l4wdvvNJBqTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Referencias\n",
        "\n",
        "[Natural Actor–Critic Algorithms](https://inria.hal.science/hal-00840470/document)\n",
        "\n",
        "[Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problems](https://www.derongliu.org/adp/adp-cdrom/Barto1983.pdf)"
      ],
      "metadata": {
        "id": "OlYr4z4-YlIz"
      }
    }
  ]
}